{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ns3Whrtm0XyP"
   },
   "source": [
    "# Explosion Explorer\n",
    "\n",
    "**If a bomb goes off in the forest, and noones around to confirm it, what's the probability that it really happened?**\n",
    "\n",
    "**By Caleb Buffa and Zach Phillips**\n",
    "\n",
    "**Explosion events recorded in databases are usually confirmed by humans. But what happens if nobody is around, or survives, to confirm the event? Is it possible to estimate the probability that a suspected explosion occurred?**\n",
    "\n",
    "**The Explosion Explorer Application gives users the ability to explore unpopulated places and examine the effects that explosions have on the surface of the Earth. Explosion Explorer leverages explosion events from the open-source Armed Conflict Location Event Database (ACLED) and before/after changes in indices derived from publicly available Landsat and Sentinel-2 products (NDVI, EVI, NBRT, and SAR) as training data for estimating the probability that an explosion occurred in a certain area, during a certain time period. Explosions included in the training dataset include shellings/artillery/missiles, air/drone strikes, landmines, remote triggered devices, suicide bombings, and grenades.**\n",
    "\n",
    "**Red points initially displayed on the map are the ACLED points for explosions. Explore a known explosion location, or select a location and date for an unconfirmed location to estimate the probability that an explosion occurred in the selected location near the selected date.**\n",
    "\n",
    "**Steps to estimate unconfirmed explosions:**\n",
    "\n",
    "1. Activate the Map by clicking the box above the `Submit` button.\n",
    "2. Use the map tools to drop a placemarker on the map\n",
    "3. Select a Date to Investigate\n",
    "4. Click the `Submit` button to estimate whether/not your point has been subjected to any explosions on the date selected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lBWe7F630XyQ"
   },
   "outputs": [],
   "source": [
    "# Check geemap installation\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    import geemap\n",
    "except ImportError:\n",
    "    print('geemap package is not installed. Installing ...')\n",
    "    subprocess.check_call([\"python\", '-m', 'pip', 'install', 'geemap'])\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import ee\n",
    "import ipywidgets as widgets\n",
    "from bqplot import pyplot as plt\n",
    "from ipyleaflet import WidgetControl\n",
    "import numpy as np\n",
    "import ipyleaflet\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "acled = pd.read_csv(r'2018-02-13-2021-02-17.csv')\n",
    "acled_sub = acled.loc[(acled['sub_event_type'] == 'Shelling/artillery/missile attack')]\n",
    "cols = ['longitude','latitude']\n",
    "coordinates = acled_sub[cols].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining backend functions for widget\n",
    "\n",
    "reducers = ee.Reducer.mean().combine(**\n",
    "                                     {'reducer2': ee.Reducer.stdDev(), \n",
    "                                      'sharedInputs': True}).combine(**\n",
    "                                                                     {'reducer2': ee.Reducer.max(), \n",
    "                                                                      'sharedInputs': True}).combine(**\n",
    "                                                                                                     {'reducer2':ee.Reducer.min(), \n",
    "                                                                                                      'sharedInputs':True}).combine(**\n",
    "                                                                                                                                    {'reducer2': ee.Reducer.percentile([25, 50, 75]), \n",
    "                                                                                                                                     'sharedInputs':True})\n",
    "def det(im):\n",
    "    return im.expression('b(0) * b(1)')\n",
    "    \n",
    "def chi2cdf(chi2, df):\n",
    "    ''' Chi square cumulative distribution function for df degrees of freedom\n",
    "    using the built-in incomplete gamma function gammainc() '''\n",
    "    return ee.Image(chi2.divide(2)).gammainc(ee.Number(df).divide(2))\n",
    "\n",
    "def sar(before, after, geometry): \n",
    "    m = 5\n",
    "    \n",
    "    try:\n",
    "        # The observed test statistic image -2logq\n",
    "        m2logq = det(before).log().add(det(after).log()).subtract(det(before.add(after)).log().multiply(2)).add(4*np.log(2)).multiply(-2*m)\n",
    "        \n",
    "        # The P value image prob(m2logQ > m2logq) = 1 - prob(m2logQ < m2logq).\n",
    "        p_value = ee.Image.constant(1).subtract(chi2cdf(m2logq, 2))\n",
    "        \n",
    "        c_map = p_value.multiply(0).where(p_value.lt(0.05), 1)\n",
    "        \n",
    "        diff = after.subtract(before) # Getting the difference between the two images\n",
    "        d_map = c_map.multiply(0)                    # Initialize the direction map to zero.\n",
    "        d_map = d_map.where(det(diff).gt(0), 1)      # All pos or neg def diffs are now labeled 1.\n",
    "        d_map = d_map.where(diff.select(0).gt(0), 2) # Re-label pos def (and label some indef) to 2.\n",
    "        d_map = d_map.where(det(diff).lt(0), 1)      # Label all indef to 1.\n",
    "        c_map = c_map.multiply(d_map) # Re-label the c_map, 0*X = 0, 1*1 = 1, 1*2= 2, 1*3 = 3.\n",
    "        \n",
    "        \n",
    "        stats = c_map.reduceRegion(**{'reducer':reducers, 'bestEffort':True, 'scale':30, 'geometry':geometry}).getInfo()\n",
    "        mean = stats['constant_mean']\n",
    "        stddev = stats['constant_stdDev']\n",
    "        p25 = stats['constant_p25']\n",
    "        p50 = stats['constant_p50']\n",
    "        p75 = stats['constant_p75']\n",
    "        mini = stats['constant_min']\n",
    "        maxi = stats['constant_max']\n",
    "        \n",
    "    except: \n",
    "        mean = None\n",
    "        stddev = None\n",
    "        p25 = None\n",
    "        p50 = None\n",
    "        p75 = None\n",
    "        mini = None\n",
    "        maxi = None\n",
    "        \n",
    "    return p25, p50, p75, mean, stddev, mini, maxi, c_map\n",
    "\n",
    "def nbr(before, after, geometry):\n",
    "    try:\n",
    "        \n",
    "        before = before.normalizedDifference(['B5', 'B7']).rename('NBR')\n",
    "        after = after.normalizedDifference(['B5', 'B7']).rename('NBR')\n",
    "        \n",
    "        \n",
    "        stats_b = before.reduceRegion(**{'reducer':reducers,'bestEffort':True, 'scale':30, 'geometry':geometry}).getInfo()    \n",
    "        \n",
    "        stats_a = after.reduceRegion(**{'reducer':reducers,'bestEffort':True, 'scale':30, 'geometry':geometry}).getInfo()\n",
    "\n",
    "        mean_b = stats_b['NBR_mean']\n",
    "        mean_a = stats_a['NBR_mean']\n",
    "\n",
    "        p25_b = stats_b['NBR_p25']\n",
    "        p25_a = stats_a['NBR_p25']\n",
    "\n",
    "        p50_b = stats_b['NBR_p50']\n",
    "        p50_a = stats_a['NBR_p50']\n",
    "\n",
    "        p75_b = stats_b['NBR_p75']\n",
    "        p75_a = stats_a['NBR_p75']\n",
    "\n",
    "        stddev_b = stats_b['NBR_stdDev']\n",
    "        stddev_a = stats_a['NBR_stdDev']\n",
    "\n",
    "        min_b = stats_b['NBR_min']\n",
    "        min_a = stats_a['NBR_min']\n",
    "\n",
    "        max_b = stats_b['NBR_max']\n",
    "        max_a = stats_a['NBR_max']\n",
    "\n",
    "        p25 = p25_a - p25_b * 100\n",
    "        p50 = p50_a - p50_b * 100\n",
    "        p75 = p75_a - p50_b * 100\n",
    "        mean = mean_a - mean_b * 100\n",
    "        mini = min_a - min_b * 100\n",
    "        maxi = max_a - max_b * 100\n",
    "        stddev = stddev_a - stddev_b * 100\n",
    "  \n",
    "    except: \n",
    "        p25 = None\n",
    "        p50 = None\n",
    "        p75 = None\n",
    "        mean = None\n",
    "        stddev = None\n",
    "        mini = None\n",
    "        maxi = None\n",
    "    \n",
    "    return p25, p50, p75, mean, stddev, mini, maxi, before, after\n",
    "\n",
    "def ndvi(before, after, geometry):\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        before = before.normalizedDifference(['B5', 'B4']).rename('NDVI')\n",
    "        \n",
    "        after = after.normalizedDifference(['B5', 'B4']).rename('NDVI')\n",
    "        \n",
    "        stats_b = before.reduceRegion(**{'reducer':reducers,'bestEffort':True, 'scale':30, 'geometry':geometry}).getInfo()\n",
    "        \n",
    "        stats_a = after.reduceRegion(**{'reducer':reducers,'bestEffort':True, 'scale':30, 'geometry':geometry}).getInfo()\n",
    "\n",
    "        mean_b = stats_b['NDVI_mean']\n",
    "        mean_a = stats_a['NDVI_mean']\n",
    "\n",
    "        p25_b = stats_b['NDVI_p25']\n",
    "        p25_a = stats_a['NDVI_p25']\n",
    "\n",
    "        p50_b = stats_b['NDVI_p50']\n",
    "        p50_a = stats_a['NDVI_p50']\n",
    "\n",
    "        p75_b = stats_b['NDVI_p75']\n",
    "        p75_a = stats_a['NDVI_p75']\n",
    "\n",
    "        stddev_b = stats_b['NDVI_stdDev']\n",
    "        stddev_a = stats_a['NDVI_stdDev']\n",
    "\n",
    "        min_b = stats_b['NDVI_min']\n",
    "        min_a = stats_a['NDVI_min']\n",
    "\n",
    "        max_b = stats_b['NDVI_max']\n",
    "        max_a = stats_a['NDVI_max']\n",
    "\n",
    "        p25 = p25_a - p25_b * 100\n",
    "        p50 = p50_a - p50_b * 100\n",
    "        p75 = p75_a - p50_b * 100\n",
    "        mean = mean_a - mean_b * 100\n",
    "        mini = min_a - min_b * 100\n",
    "        maxi = max_a - max_b * 100\n",
    "        stddev = stddev_a - stddev_b * 100 \n",
    "  \n",
    "    except: \n",
    "        p25 = None\n",
    "        p50 = None\n",
    "        p75 = None\n",
    "        mean = None\n",
    "        stddev = None\n",
    "        mini = None\n",
    "        maxi = None\n",
    "    \n",
    "    return p25, p50, p75, mean, stddev, mini, maxi, before, after\n",
    "\n",
    "def evi(before, after, geometry):\n",
    "    try:\n",
    "        before_evi = before.expression(\n",
    "            '2.5 * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))', {\n",
    "                'NIR': before.select('B5'),\n",
    "                'RED': before.select('B4'),\n",
    "                'BLUE': before.select('B2')})\n",
    "\n",
    "        after_evi = after.expression(\n",
    "            '2.5 * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))', {\n",
    "                'NIR': after.select('B5'),\n",
    "                'RED': after.select('B4'),\n",
    "                'BLUE': after.select('B2')})\n",
    "\n",
    "        stats_b = before_evi.rename('EVI').reduceRegion(**{'reducer':reducers, 'bestEffort':True, 'scale':30, 'geometry':geometry}).getInfo()\n",
    "        stats_a = after_evi.rename('EVI').reduceRegion(**{'reducer':reducers, 'bestEffort':True, 'scale':30, 'geometry':geometry}).getInfo()\n",
    "\n",
    "        mean_b = stats_b['EVI_mean']\n",
    "        mean_a = stats_a['EVI_mean']\n",
    "\n",
    "        p25_b = stats_b['EVI_p25']\n",
    "        p25_a = stats_a['EVI_p25']\n",
    "\n",
    "        p50_b = stats_b['EVI_p50']\n",
    "        p50_a = stats_a['EVI_p50']\n",
    "\n",
    "        p75_b = stats_b['EVI_p75']\n",
    "        p75_a = stats_a['EVI_p75']\n",
    "\n",
    "        stddev_b = stats_b['EVI_stdDev']\n",
    "        stddev_a = stats_a['EVI_stdDev']\n",
    "\n",
    "        min_b = stats_b['EVI_min']\n",
    "        min_a = stats_a['EVI_min']\n",
    "\n",
    "        max_b = stats_b['EVI_max']\n",
    "        max_a = stats_a['EVI_max']\n",
    "\n",
    "        p25 = p25_a - p25_b * 100\n",
    "        p50 = p50_a - p50_b * 100\n",
    "        p75 = p75_a - p50_b * 100\n",
    "        mean = mean_a - mean_b * 100\n",
    "        mini = min_a - min_b * 100\n",
    "        maxi = max_a - max_b * 100\n",
    "        stddev = stddev_a - stddev_b * 100\n",
    "  \n",
    "    except: \n",
    "        p25 = None\n",
    "        p50 = None\n",
    "        p75 = None\n",
    "        mean = None\n",
    "        stddev = None\n",
    "        mini = None\n",
    "        maxi = None\n",
    "    \n",
    "    return p25, p50, p75, mean, stddev, mini, maxi, before_evi, after_evi\n",
    "\n",
    "def difference(coordinate, date):\n",
    "    \n",
    "    point = ee.Geometry.Point(coordinate.getInfo()['features'][0]['geometry']['coordinates'])\n",
    "    geometry = point.buffer(1000) # buffers point by 1 km\n",
    "\n",
    "    md = ee.Date(date)\n",
    "    sd = md.advance(-4, 'week')\n",
    "    ed = md.advance(4, 'week')\n",
    "\n",
    "    before_sar = ee.ImageCollection(\"COPERNICUS/S1_GRD_FLOAT\").filterBounds(point).filterDate(sd, md).filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING')).limit(1, 'system:time_start', False).first() \n",
    "    after_sar = ee.ImageCollection(\"COPERNICUS/S1_GRD_FLOAT\").filterBounds(point).filterDate(md, ed).filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING')).first()\n",
    "\n",
    "        \n",
    "    before_landsat = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR').filterBounds(point).filterDate(sd, md).sort('CLOUD_COVER').limit(1, 'system:time_start', False).first()\n",
    "    after_landsat = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR').filterBounds(point).filterDate(md, ed).sort('CLOUD_COVER').first()\n",
    "    \n",
    "    ndvi_p25, ndvi_p50, ndvi_p75, ndvi_mean, ndvi_stddev, ndvi_min, ndvi_max, before_ndvi, after_ndvi = ndvi(before_landsat, \n",
    "                                                                                                             after_landsat,\n",
    "                                                                                                             geometry)  \n",
    "\n",
    "    evi_p25, evi_p50, evi_p75, evi_mean, evi_stddev, evi_min, evi_max, before_evi, after_evi = evi(before_landsat, \n",
    "                                                                                                   after_landsat, \n",
    "                                                                                                   geometry)\n",
    "\n",
    "    sar_p25, sar_p50, sar_p75, sar_mean, sar_stddev, sar_min, sar_max, c_map = sar(before_sar,\n",
    "                                                                                   after_sar, \n",
    "                                                                                   geometry)\n",
    "\n",
    "    nbr_p25_, nbr_p50, nbr_p75, nbr_mean, nbr_stddev, nbr_min, nbr_max, before_nbr, after_nbr = nbr(before_landsat,\n",
    "                                                                                                    after_landsat, \n",
    "                                                                                                    geometry)\n",
    "\n",
    "    \n",
    "    ar = np.array([ndvi_p25, ndvi_p50, ndvi_p75, ndvi_mean, ndvi_min, ndvi_max, \n",
    "                   evi_p25, evi_p50, evi_p75, evi_mean, evi_stddev, evi_min, evi_max, \n",
    "                   sar_mean, sar_stddev, sar_max, \n",
    "                   nbr_mean, nbr_stddev, nbr_p25_, nbr_p50, nbr_p75, nbr_min]).reshape(1,-1)\n",
    "    \n",
    "    try:\n",
    "        pred = clf.predict(ar)\n",
    "        prob = clf.predict_proba(ar)\n",
    "        if pred[0] == 1:\n",
    "            pred = f'Explosion Likely Occured On: {date}'\n",
    "            prob = f\"Probability of Explosion: {round(prob[0][1]*100, 2)}%\"\n",
    "        else:\n",
    "            pred = f'Explosion Did Not Likely Occur On: {date}'\n",
    "            prob = f'Probability of No Explosion: {round(prob[0][0]*100, 2)}%'\n",
    "        print(f\"{pred}\\n{prob}\")\n",
    "    except:\n",
    "        print('Error: Please choose different location/date')\n",
    "\n",
    "    try:\n",
    "        Map.addLayer(before_ndvi.clip(geometry), {'min':0, 'max':1, \"palette\": [\n",
    "        'FFFFFF', 'CE7E45', 'DF923D', 'F1B555', 'FCD163', '99B718', '74A901',\n",
    "        '66A000', '529400', '3E8601', '207401', '056201', '004C00', '023B01',\n",
    "        '012E01', '011D01', '011301']}, 'Before NDVI')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        Map.addLayer(after_ndvi.clip(geometry), {'min':0, 'max':1, \"palette\": [\n",
    "        'FFFFFF', 'CE7E45', 'DF923D', 'F1B555', 'FCD163', '99B718', '74A901',\n",
    "        '66A000', '529400', '3E8601', '207401', '056201', '004C00', '023B01',\n",
    "        '012E01', '011D01', '011301']}, 'After NDVI')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        Map.addLayer(before_evi.clip(geometry), {'min':0, 'max':1, 'palette': [\n",
    "        'FFFFFF', 'CE7E45', 'DF923D', 'F1B555', 'FCD163', '99B718', '74A901',\n",
    "        '66A000', '529400', '3E8601', '207401', '056201', '004C00', '023B01',\n",
    "        '012E01', '011D01', '011301']}, 'Before EVI')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        Map.addLayer(after_evi.clip(geometry), {'min':0, 'max':1, \"palette\": [\n",
    "        'FFFFFF', 'CE7E45', 'DF923D', 'F1B555', 'FCD163', '99B718', '74A901',\n",
    "        '66A000', '529400', '3E8601', '207401', '056201', '004C00', '023B01',\n",
    "        '012E01', '011D01', '011301']}, 'After EVI')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        Map.addLayer(before_nbr.clip(geometry), {'min':0, 'max':1, 'palette':['000000', 'FFFFFF']}, 'Before NBR')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        Map.addLayer(after_nbr.clip(geometry), {'min':0, 'max':1, 'palette':['000000', 'FFFFFF']}, 'After NBR')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        Map.addLayer(c_map.clip(geometry), {'palette':['white', 'blue', 'red']}, 'SAR Probability')\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "# Loading in trained classifier\n",
    "filename = 'explosion_model.sav'\n",
    "clf = pickle.load(open(filename, 'rb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iyZH8Z3L0XyR",
    "outputId": "0eeed34c-46b2-43fd-bd4a-092d0c6f79ff"
   },
   "outputs": [],
   "source": [
    "# Create an interactive map\n",
    "Map = geemap.Map(center=[0,0], zoom=2, add_google_map=False)\n",
    "Map.add_basemap('HYBRID')\n",
    "\n",
    "sentinel = ee.ImageCollection('COPERNICUS/S1_GRD_FLOAT')\n",
    "landsat = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\n",
    "\n",
    "# Adding date widget to map\n",
    "date_picker = widgets.DatePicker(description='Pick a Date', disabled=False)\n",
    "date_widget = widgets.Output(layout={'border': '1px solid black'})\n",
    "date_control = ipyleaflet.WidgetControl(widget=date_widget, position='bottomright')\n",
    "Map.add_control(date_control) \n",
    "\n",
    "with date_widget:\n",
    "    display(date_picker)\n",
    "    \n",
    "prediction_widget = widgets.Output(layout={'border': '1px solid black'})\n",
    "prediction_control = ipyleaflet.WidgetControl(widget=prediction_widget, position='topright')\n",
    "Map.add_control(prediction_control)\n",
    "\n",
    "style = {'description_width': 'initial'}\n",
    "\n",
    "submit = widgets.Button(description='Submit',button_style='primary',\n",
    "                        tooltip='Click me',style=style)\n",
    "\n",
    "aoi_widget = widgets.Checkbox(value=False, \n",
    "                              description='Drop point, pick date, and click Submit',style=style)\n",
    "full_widget = widgets.VBox([widgets.HBox([aoi_widget]),submit])\n",
    "\n",
    "full_control = ipyleaflet.WidgetControl(widget=full_widget, position='bottomright')\n",
    "Map.add_control(full_control)\n",
    "\n",
    "def submit_clicked(b):\n",
    "    with prediction_widget:\n",
    "        prediction_widget.clear_output()\n",
    "        print('Computing...')\n",
    "#        Map.default_style = {'cursor': 'wait'}\n",
    "        try:\n",
    "            roi = ee.FeatureCollection(Map.draw_last_feature)\n",
    "            date = str(date_picker.value)           \n",
    "            if roi:\n",
    "                if Map.draw_last_feature is not None:\n",
    "                    roi = ee.FeatureCollection(Map.draw_last_feature)                    \n",
    "                    if date_picker.value is not None:\n",
    "                        difference(roi, date)\n",
    "                    else:\n",
    "                        pass                    \n",
    "                else:\n",
    "                    pass                \n",
    "            else:\n",
    "                pass        \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('An Error Occured During Computation')\n",
    "submit.on_click(submit_clicked)\n",
    "\n",
    "cords = []\n",
    "for c in coordinates:    \n",
    "    point = ee.Geometry.Point(c[0], c[1])\n",
    "    cords.append(point)\n",
    "fc = ee.FeatureCollection(cords)\n",
    "Map.addLayer(fc, {'color':'Red'}, 'ACLED Explosions')\n",
    "\n",
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "app_frontend.ipynb",
   "provenance": []
  },
  "gist": {
   "data": {
    "description": "notebooks/wetland_mapping.ipynb",
    "public": true
   },
   "id": ""
  },
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
